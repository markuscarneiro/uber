{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac65f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports e configurações\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 120\n",
    "pd.options.display.width = 180\n",
    "import warnings \n",
    "from dateutil import parser\n",
    "from typing import Iterable, Optional\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Leitura e concatenação dos arquivos (com dtype=str para evitar inferência incorreta)\n",
    "folder_path = r\"C:\\Users\\u8178\\OneDrive - GRUPO EQUATORIAL ENERGIA\\99 - QUASAR\\Uber\\parent\"\n",
    "csv_paths = []\n",
    "for fn in os.listdir(folder_path):\n",
    "    p = os.path.join(folder_path, fn)\n",
    "    if os.path.isfile(p) and fn.lower().endswith(('.csv','.xlsx','.xls')):\n",
    "        csv_paths.append(p)\n",
    "all_dfs = []\n",
    "for path in csv_paths:\n",
    "    if path.lower().endswith('.csv'):\n",
    "        tmp = pd.read_csv(path, header=4, sep=';', dtype=str)\n",
    "    else:\n",
    "        tmp = pd.read_excel(path, header=5, dtype=str)\n",
    "    all_dfs.append(tmp)\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06569d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b. Dicionário de renomeação (mantido como referência)\n",
    "new_column_names = {\n",
    "    'UUID da organização': 'uuid_organizacao',\n",
    "    'Nome da organização': 'nome_organizacao',\n",
    "    'Moeda local': 'moeda_local',\n",
    "    'ID da viagem/Uber Eats': 'id_viagem',\n",
    "    'Registro de data e hora da transação (UTC)': 'data_hora_transacao_utc',\n",
    "    'Data da solicitação (UTC)': 'data_solicitacao_utc',\n",
    "    'Hora da solicitação (UTC)': 'hora_solicitacao_utc',\n",
    "    'Data da solicitação (local)': 'data_solicitacao_local',\n",
    "    'Hora da solicitação (local)': 'hora_solicitacao_local',\n",
    "    'Data de chegada (UTC)': 'data_chegada_utc',\n",
    "    'Hora de chegada (UTC)': 'hora_chegada_utc',\n",
    "    'Data de chegada (local)': 'data_chegada_local',\n",
    "    'Hora de chegada (local)': 'hora_chegada_local',\n",
    "    'Compensação do fuso horário de solicitação a partir do UTC': 'comp_fuso_horario_utc',\n",
    "    'Nome': 'nome',\n",
    "    'Sobrenome': 'sobrenome',\n",
    "    'E-mail': 'email',\n",
    "    'ID do funcionário': 'id_funcionario',\n",
    "    'Serviço': 'servico',\n",
    "    'Cidade': 'cidade',\n",
    "    'Distância (mi)': 'distancia_mi',\n",
    "    'Duração (min)': 'duracao_min',\n",
    "    'Endereço de partida': 'endereco_partida',\n",
    "    'Endereço de destino': 'endereco_destino',\n",
    "    'Código da despesa': 'codigo_despesa',\n",
    "    'Detalhamento da despesa': 'detalhamento_despesa',\n",
    "    'Faturas': 'faturas',\n",
    "    'Programa': 'programa',\n",
    "    'Grupo': 'grupo',\n",
    "    'Forma de pagamento': 'forma_pagamento',\n",
    "    'Tipo de transação': 'tipo_transacao',\n",
    "    'Valor na moeda local (sem tributos)': 'valor_local_sem_tributos',\n",
    "    'Tributos na moeda local': 'tributos_local',\n",
    "    'Valor extra em moeda local': 'valor_extra_local',\n",
    "    'Valor da transação na moeda local (com tributos)': 'valor_transacao_local_com_tributos',\n",
    "    'Código da moeda local': 'codigo_moeda_local',\n",
    "    'Valor na moeda local (sem impostos)': 'valor_local_sem_impostos',\n",
    "    'Impostos na moeda local': 'impostos_local',\n",
    "    'Valor extra na moeda local': 'valor_extra_local_dup',\n",
    "    'Valor da transação na moeda local (com impostos)': 'valor_transacao_local_com_impostos',\n",
    "    'Taxa de serviço e tecnologia estimada (com tributos aplicáveis) na moeda local': 'taxa_servico_tecnologia_local',\n",
    "    'Número da fatura': 'numero_fatura',\n",
    "    'Deduções em moeda local': 'deducoes_local',\n",
    "    'IsGroupOrder': 'eh_pedido_grupo',\n",
    "    'Tipo de atendimento': 'tipo_atendimento',\n",
    "    'País': 'pais',\n",
    "    'Tipo de cancelamento': 'tipo_cancelamento',\n",
    "    'Economia pela assinatura (moeda local)': 'economia_assinatura_local'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba35b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c. Renomeia as colunas conforme o dicionário\n",
    "df = df.rename(columns=new_column_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: Listar os campos que você deseja alterar\n",
    "colunas_para_alterar = ['hora_solicitacao_utc', 'hora_solicitacao_local', 'hora_chegada_utc', 'hora_chegada_local']\n",
    "\n",
    "# Função para converter horários em objetos datetime.time\n",
    "def converter_horario(horario):\n",
    "    try:\n",
    "        # Tenta interpretar o horário automaticamente\n",
    "        return parser.parse(horario).time()\n",
    "    except Exception:\n",
    "        # Retorna NaT caso o valor seja inválido ou vazio\n",
    "        return pd.NaT\n",
    "\n",
    "# Passo 2: Aplicar a conversão nas colunas listadas diretamente no DataFrame original\n",
    "df[colunas_para_alterar] = df[colunas_para_alterar].applymap(converter_horario)\n",
    "\n",
    "# Exibir o DataFrame atualizado\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73682b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_iso_date(df: pd.DataFrame,\n",
    "                                cols: Iterable[str],\n",
    "                                inplace: bool = True,\n",
    "                                as_string: bool = True,\n",
    "                                infer_fallback: bool = True,\n",
    "                                na_value: Optional[str] = None) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Converte colunas de df com datas no padrão MM/DD/YYYY (ou timestamps)\n",
    "    para o padrão ISO YYYY-MM-DD.\n",
    "\n",
    "    Parâmetros\n",
    "    - df: DataFrame que contém as colunas.\n",
    "    - cols: lista/iterável de nomes de colunas a converter.\n",
    "    - inplace: se True altera df no lugar; se False retorna uma cópia modificada.\n",
    "    - as_string: se True escreve strings 'YYYY-MM-DD'; se False deixa dtype datetime64[ns].\n",
    "    - infer_fallback: se True, para valores não parseados com o formato exato\n",
    "                      tenta pd.to_datetime (inferência) como fallback.\n",
    "    - na_value: valor a usar quando a data é inválida e as_string=True (ex.: None, '', 'NA').\n",
    "                Ignorado se as_string=False (fica NaT).\n",
    "    Retorna\n",
    "    - None se inplace=True; caso contrário retorna o DataFrame convertido.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            print(f\"[warn] coluna '{c}' não encontrada — pulando\")\n",
    "            continue\n",
    "\n",
    "        ser = df[c]\n",
    "        # Se já for datetime, usamos direto\n",
    "        if pd.api.types.is_datetime64_any_dtype(ser):\n",
    "            dt = ser.dt.normalize()  # manter só a data\n",
    "        else:\n",
    "            ser_str = ser.astype(str).str.strip().replace({'nan': None, 'None': None, '': None})\n",
    "\n",
    "            # tentativa estrita para MM/DD/YYYY (mais rápida e determinística)\n",
    "            dt = pd.to_datetime(ser_str, format=\"%m/%d/%Y\", errors=\"coerce\", dayfirst=False)\n",
    "\n",
    "            # fallback permissivo apenas onde necessário\n",
    "            if infer_fallback:\n",
    "                mask_na = dt.isna() & ser_str.notna()\n",
    "                if mask_na.any():\n",
    "                    dt_fallback = pd.to_datetime(ser_str[mask_na], errors=\"coerce\", infer_datetime_format=True, dayfirst=False)\n",
    "                    dt.loc[mask_na] = dt_fallback\n",
    "\n",
    "            # se houver strings como \"2025-03-15 10:00:00\" ou com timezone, o fallback captura\n",
    "            # garantimos apenas a parte de data (normalizar)\n",
    "            dt = dt.dt.normalize()\n",
    "\n",
    "        # salvar no formato desejado\n",
    "        if as_string:\n",
    "            out = dt.dt.strftime(\"%Y-%m-%d\")\n",
    "            # strftime produz NaN para NaT; padronizamos para na_value (ou None)\n",
    "            if na_value is None:\n",
    "                out = out.where(~dt.isna(), None)\n",
    "            else:\n",
    "                out = out.where(~dt.isna(), na_value)\n",
    "            df[c] = out\n",
    "        else:\n",
    "            df[c] = dt  # datetime64[ns]\n",
    "\n",
    "        # resumo rápido por coluna\n",
    "        n_total = len(df)\n",
    "        n_valid = df[c].notna().sum() if as_string else df[c].notna().sum()\n",
    "        n_invalid = n_total - n_valid\n",
    "        print(f\"[ok] coluna '{c}': total={n_total}, válidos={n_valid}, inválidos={n_invalid}\")\n",
    "\n",
    "    if not inplace:\n",
    "        return df\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- uso direto (executar abaixo desta célula) -----------------\n",
    "# Exemplos de chamada (ajuste se quiser as strings ou datetime dtype):\n",
    "# Converte as colunas indicadas em-place e grava strings 'YYYY-MM-DD' (None para inválidos)\n",
    "# convert_columns_to_iso_date(df, ['data_solicitacao_utc','data_solicitacao_local','data_chegada_utc','data_chegada_local'],\n",
    "#                             inplace=True, as_string=True, infer_fallback=True, na_value=None)\n",
    "\n",
    "# Se preferir manter dtype datetime (recomendado para manipulações posteriores):\n",
    "convert_columns_to_iso_date(df, ['data_solicitacao_utc','data_solicitacao_local','data_chegada_utc','data_chegada_local'], inplace=True, as_string=False)\n",
    "\n",
    "# Obs: se quiser que a função retorne um novo DataFrame em vez de alterar o existente,\n",
    "# use inplace=False e capture o retorno:\n",
    "# df2 = convert_columns_to_iso_date(df, [...], inplace=False, as_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4557a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_hora_transacao_utc'] = pd.to_datetime(df['data_hora_transacao_utc'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff219b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['data_solicitacao_utc','data_solicitacao_local','data_chegada_utc','data_chegada_local']\n",
    "\n",
    "# linhas onde qualquer coluna está NA/NaT\n",
    "invalid_any = df[df[cols].isna().any(axis=1)].copy()\n",
    "\n",
    "# resumo rápido\n",
    "print(\"Total registros:\", len(df))\n",
    "print(\"Registros com qualquer data inválida:\", len(invalid_any))\n",
    "print(\"Contagem inválidos por coluna:\")\n",
    "print(df[cols].isna().sum())\n",
    "# mostrar primeiras 10 linhas para inspeção\n",
    "invalid_any.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_any.to_csv('erros_conversao_data.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=invalid_any.index)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import = df[['nome_organizacao', 'id_viagem', 'data_hora_transacao_utc', 'data_solicitacao_utc', 'hora_solicitacao_utc', 'data_solicitacao_local', 'hora_solicitacao_local', 'data_chegada_utc', 'hora_chegada_utc', 'data_chegada_local', 'hora_chegada_local', 'nome', 'sobrenome', 'email', 'id_funcionario', 'servico', 'cidade', 'distancia_mi', 'duracao_min', 'endereco_partida', 'endereco_destino', 'codigo_despesa', 'detalhamento_despesa', 'programa', 'grupo','valor_transacao_local_com_tributos','valor_extra_local', 'deducoes_local']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8996e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c605cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento de mês para abreviação em português\n",
    "mes_abrev = {\n",
    "    'January': 'jan', 'February': 'fev', 'March': 'mar', 'April': 'abr',\n",
    "    'May': 'mai', 'June': 'jun', 'July': 'jul', 'August': 'ago',\n",
    "    'September': 'set', 'October': 'out', 'November': 'nov', 'December': 'dez'\n",
    "}\n",
    "\n",
    "df_import['mes_solicitacao'] = df_import['data_solicitacao_local'].dt.month_name().map(mes_abrev)\n",
    "\n",
    "# Move a coluna 'mes_solicitacao' para logo após 'data_solicitacao_local'\n",
    "cols = list(df_import.columns)\n",
    "cols.remove('mes_solicitacao')\n",
    "idx = cols.index('data_solicitacao_local') + 1\n",
    "cols = cols[:idx] + ['mes_solicitacao'] + cols[idx:]\n",
    "df_import = df_import[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import['dia_da_semana_dt_solicitacao'] = df_import['data_solicitacao_local'].dt.day_name(locale='pt_BR')\n",
    "\n",
    "cols = list(df_import.columns)\n",
    "cols.remove('dia_da_semana_dt_solicitacao')\n",
    "idx = cols.index('mes_solicitacao') + 1\n",
    "cols = cols[:idx] + ['dia_da_semana_dt_solicitacao'] + cols[idx:]\n",
    "df_import = df_import[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44368004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time\n",
    "\n",
    "def classificar_hora(hora_str):\n",
    "    if pd.isna(hora_str):\n",
    "        return None\n",
    "    try:\n",
    "        h, m, s = map(int, str(hora_str).split(':'))\n",
    "        hora = time(h, m, s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    if time(8,0,0) <= hora <= time(12,0,0):\n",
    "        return 'expediente_manha'\n",
    "    elif time(12,1,0) <= hora <= time(14,0,0):\n",
    "        return 'horario_almoco'\n",
    "    elif time(14,1,0) <= hora <= time(18,0,0):\n",
    "        return 'expediente_tarde'\n",
    "    elif time(18,0,1) <= hora <= time(20,0,0):\n",
    "        return 'entre 18h e 20h'\n",
    "    elif time(20,0,1) <= hora <= time(23,59,0):\n",
    "        return 'entre 20h e 23h59'\n",
    "    elif (time(0,0,0) <= hora <= time(6,0,0)):\n",
    "        return 'entre 00h e 06h'\n",
    "    elif time(6,1,0) <= hora <= time(8,0,0):\n",
    "        return 'entre 06h e 08h'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_import['classif_hora_local'] = df_import['hora_solicitacao_local'].apply(classificar_hora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f77d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df_import.columns)\n",
    "cols.remove('classif_hora_local')\n",
    "idx = cols.index('hora_solicitacao_local') + 1\n",
    "cols = cols[:idx] + ['classif_hora_local'] + cols[idx:]\n",
    "df_import = df_import[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte colunas para float, tratando vírgula como separador decimal\n",
    "for col in ['valor_transacao_local_com_tributos', 'valor_extra_local']:\n",
    "    df_import[col] = (df_import[col].astype(str).str.replace(',', '.', regex=False).str.replace(r'[^0-9\\.-]', '', regex=True).astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dc83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove caracteres não numéricos e ponto, substitui strings vazias ou '--' por NaN, converte para float e preenche nulos com 0\n",
    "df_import['deducoes_local'] = (\n",
    "    df_import['deducoes_local']\n",
    "    .astype(str)\n",
    "    .str.replace(r'[^0-9\\.-]', '', regex=True)\n",
    "    .replace(['', '--'], np.nan)\n",
    "    .astype(float)\n",
    "    .fillna(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_import"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
